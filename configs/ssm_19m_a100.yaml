# SSM-ASR Training Configuration
# Target: ~19M parameter model (similar to ABR asr-19m-v2-en-32b)
# Hardware: 2x A100 80GB GPUs
# Training data: ~15,000 hours of English speech

# Model Architecture
model:
  type: ssm
  d_model: 256
  n_layers: 18
  d_state: 64
  d_conv: 4
  expand: 2
  headdim: 64
  attention_every_n_layers: 4
  bidirectional: true
  chunk_size: 256
  vocab_size: 256  # SentencePiece vocab size
  n_mels: 80
  dropout: 0.1
  decoder_type: ctc

  # Frontend
  conv_channels: [256, 256]
  conv_kernel_sizes: [3, 3]
  conv_strides: [2, 2]  # 4x downsampling

# Feature Extraction
features:
  type: mel_spectrogram
  sample_rate: 16000
  n_mels: 80
  n_fft: 400
  hop_length: 160
  win_length: 400
  f_min: 0.0
  f_max: 8000.0
  normalize: true
  log_mel: true

# Tokenizer
tokenizer:
  type: sentencepiece
  model_path: tokenizers/spm_256.model
  vocab_size: 256

# Training
training:
  # Optimization
  epochs: 100
  batch_size: 256  # Per GPU
  gradient_accumulation_steps: 1  # Effective batch = 256 * 1 * 2 GPUs = 512
  learning_rate: 5.0e-4
  min_learning_rate: 1.0e-6
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Learning rate schedule
  warmup_steps: 10000
  lr_scheduler: cosine

  # Precision
  precision: bf16
  compile: false  # Disabled for faster startup (enable for production)

  # Regularization
  label_smoothing: 0.1
  spec_augment: true

  # Checkpointing
  save_steps: 5000
  eval_steps: 2500
  logging_steps: 100

  # Output
  output_dir: /data/razhan/15k_hours/outputs/ssm_19m_en

  # Reproducibility
  seed: 42
  deterministic: true

# Distributed Training
distributed:
  strategy: ddp  # DistributedDataParallel
  backend: nccl
  find_unused_parameters: false
  gradient_as_bucket_view: true
  static_graph: true

# Data
data:
  # Training manifests (add more as they're prepared with prepare_data.py)
  train_manifests:
    - /data/razhan/15k_hours/librispeech_clean_100/manifests/librispeech_clean_100_train.json
    # - /data/razhan/15k_hours/librispeech_clean_360/manifests/librispeech_clean_360_train.json
    # - /data/razhan/15k_hours/librispeech_other/manifests/librispeech_other_train.json
    # - /data/razhan/15k_hours/voxpopuli/manifests/voxpopuli_train.json
    # - /data/razhan/15k_hours/common_voice/manifests/common_voice_train.json
    # - /data/razhan/15k_hours/tedlium/manifests/tedlium_train.json
    # - /data/razhan/15k_hours/gigaspeech/manifests/gigaspeech_train.json
    # - /data/razhan/15k_hours/ami/manifests/ami_train.json
    # - /data/razhan/15k_hours/spgispeech/manifests/spgispeech_train.json
    # - /data/razhan/15k_hours/earnings22/manifests/earnings22_train.json
    # - /data/razhan/15k_hours/peoples_speech/manifests/peoples_speech_train.json

  # Validation manifests
  val_manifests:
    - /data/razhan/15k_hours/librispeech_clean_100/manifests/librispeech_clean_100_dev.json

  # Test manifests
  test_manifests:
    - /data/razhan/15k_hours/librispeech_clean_100/manifests/librispeech_clean_100_test.json

  # Data filtering
  max_duration: 30.0  # seconds
  min_duration: 0.5   # seconds
  max_text_length: 512

  # Dataset weights (optional, for balanced sampling)
  dataset_weights:
    librispeech_clean: 1.5
    librispeech_other: 1.2
    gigaspeech: 1.0
    voxpopuli: 1.0
    common_voice: 0.8
    tedlium: 1.2
    ami_ihm: 1.5
    spgispeech: 1.0
    earnings22: 1.5

# Data Augmentation
augmentation:
  spec_augment:
    enabled: true
    freq_mask_param: 27
    time_mask_param: 100
    num_freq_masks: 2
    num_time_masks: 10

  noise_injection:
    enabled: false
    snr_min: 10
    snr_max: 30

  speed_perturbation:
    enabled: true
    speeds: [0.9, 1.0, 1.1]

# Logging
logging:
  wandb:
    enabled: true
    project: ssm-asr
    entity: null
    name: ssm_19m_en_15k
    tags: ["ssm", "mamba2", "english", "15k-hours"]

  tensorboard:
    enabled: true
    log_dir: /data/razhan/15k_hours/outputs/ssm_19m_en/tensorboard
